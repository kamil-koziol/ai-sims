from __future__ import annotations
from typing import TYPE_CHECKING
from dataclasses import dataclass
from agents.memory import Action, MemoryNodeFactory
from agents.actions.retrieve import get_string_memories
from llm_model import ModelService

if TYPE_CHECKING:
    from agents import Agent


@dataclass
class ConversationVariables:
    """
    Variables necessary for creating a conversation templates.
    """
    init_agent_name: str
    target_agent_name: str
    init_agent_description: str
    target_agent_description: str
    init_agent_action: str
    target_agent_action: str
    location: str
    init_agent_memories: str
    target_agent_memories: str

@dataclass
class SummarizeConversationVariables:
    """
    Variables necessary for creating a summarization templates.
    """
    conversation: str
    init_agent_name: str
    target_agent_name: str

@dataclass
class MemoryOnConversationVariables:
    """
    Variables necessary for creating a memory out of conversation.
    """
    conversation: str
    agent_name: str

@dataclass
class DecideToConverseVariables:
    """
    Variables necessary for deciding whether to start a conversation.
    """
    init_agent_name: str
    target_agent_name: str
    init_agent_description: str
    target_agent_description: str
    init_agent_action: str
    target_agent_action: str
    location: str
    last_convo_summary: str


def converse(init_agent: Agent, target_agent: Agent):
    """
    Decide whether to start a conversation, and if so, create a memory node of conversation
    between init agent and target agent and add it to memory stream.

    Args:
        init_agent (Agent): The agent who initialized a conversation
        target_agent (Agent): The target of initialized conversation
    """
    if not _decide_to_converse(init_agent, target_agent):
        return

    convo = generate_conversation(init_agent, target_agent)
    convo_summary = generate_conversation_summary(init_agent, target_agent, convo)

    insert_convo_into_mem_stream(init_agent, convo, convo_summary)
    insert_convo_into_mem_stream(target_agent, convo, convo_summary)

    init_agent.stm.action = Action.CONVERSING
    target_agent.stm.action = Action.CONVERSING


def generate_conversation(init_agent: Agent, target_agent: Agent) -> str:
    """
    Create a text of conversation between agents.

    Args:
        init_agent (Agent): The agent who initialized a conversation
        target_agent (Agent): The target of initialized conversation
    """
    prompt_template_file = "create_conversation.txt"
    init_agent_memories = get_string_memories(init_agent, target_agent.stm.name)
    target_agent_memories = get_string_memories(target_agent, init_agent.stm.name)
    prompt_variables = ConversationVariables(
        init_agent_name=init_agent.stm.name,
        target_agent_name=target_agent.stm.name,
        init_agent_description=init_agent.stm.description,
        target_agent_description=target_agent.stm.description,
        init_agent_action=init_agent.stm.action.value,
        target_agent_action=target_agent.stm.action.value,
        location=init_agent.stm.curr_location,
        init_agent_memories=init_agent_memories,
        target_agent_memories=target_agent_memories
    )
    output = ModelService().generate_text(prompt_variables, prompt_template_file)
    return output


def generate_conversation_summary(init_agent: Agent, target_agent: Agent, convo: str) -> str:
    """
    Generate summary of conversation.

    Args:
        init_agent (Agent): Agent that initialized conversation.
        target_agent (Agent): Agent that is target of conversation.
        convo (str): Text of conversation.

    Returns:
        str: the summary generated by llm
    """
    prompt_template_file = "summarize_conversation.txt"
    prompt_variables = SummarizeConversationVariables(
        conversation=convo,
        init_agent_name=init_agent.stm.name,
        target_agent_name=target_agent.stm.name
    )
    output = ModelService().generate_text(prompt_variables, prompt_template_file)
    return output


def generate_memory_on_conversation(agent: Agent, convo: str) -> str:
    """
    Create a memory node for an agent.

    Args:
        agent (Agent): Agent for memory node.
        convo (str): Text of the conversation.

    Returns:
        str: Description of the memory
    """
    prompt_template_file = "memo_on_convo.txt"
    prompt_variables = MemoryOnConversationVariables(
        conversation=convo,
        agent_name=agent.stm.name
    )
    output = ModelService().generate_text(prompt_variables, prompt_template_file)
    return output


def insert_convo_into_mem_stream(agent: Agent, convo: str, summary: str) -> None:
    """
    Add a memory into agent's memory stream.

    Args:
        agent (Agent): Agent to add a memory.
        convo (str): Description of conversation.
        summary (str): Summary of conversation.
    """
    dialog_node = MemoryNodeFactory.create_chat(summary, agent)
    agent.memory_stream.add_memory_node(dialog_node)

    memory = generate_memory_on_conversation(agent, convo)
    memory_node = MemoryNodeFactory.create_thought(memory, agent)
    agent.memory_stream.add_memory_node(memory_node)

def _decide_to_converse(init_agent: Agent, target_agent: Agent) -> bool:
    """
    Decide if the agent should start a conversation with the other agent.

    Args:
        target_agent (Agent): Agent to potentially converse with.

    Returns:
        True if they should converse, False if not.
    """
    prompt_template_file = "decide_to_converse.txt"
    # TODO retrieve their last convo summary (memory_node node_type==MemoryType.CHAT)
    last_convo_summary = ""
    prompt_variables = DecideToConverseVariables(
        init_agent_name=init_agent.stm.name,
        target_agent_name=target_agent.stm.name,
        init_agent_description=init_agent.stm.description,
        target_agent_description=target_agent.stm.description,
        init_agent_action=init_agent.stm.action.value,
        target_agent_action=target_agent.stm.action.value,
        location=init_agent.stm.curr_location,
        last_convo_summary=last_convo_summary
    )
    model_output = ModelService().generate_text(prompt_variables, prompt_template_file)
    decision = _convert_model_response_to_bool(model_output)
    return decision

def _convert_model_response_to_bool(response: str) -> bool:
    """
    Convert model response from string to bool value.

    Args:
        response (str): Model's response.

    Returns:
        int: Converted bool.
    """
    if "no" in response.lower():
        return False
    else:
        return True
